---
title: "Final project Group 7"
author: "Bharath Reddy Madi"
date: "2022-12-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 

## 1. Data-driven analytics for HR attrition prediction

```{r}
# data wrangling

library(dplyr)
library(magrittr)
library(stringr)
library(stringi)
library(readr)

# machine learning and advanced analytics

library(DMwR2)
library(caret)
library(caretEnsemble)
library(pROC)

# natural language processing

#library(msLanguageR)
#library(tm)
#library(jiebaR)

# tools

library(httr)
#library(XML)
#library(jsonlite)

# data visualization

library(scales)
library(ggplot2)
library(wordcloud)
     
```

```{r}
DATA1 <- "/Employee_attrition.csv"
```

```{r}
df <- read_csv("Employee_attrition.csv")
     
```

```{r}
head(df)

```

```{r}
dim(df)
```

```{r}
names(df)

```

```{r}

str(df)
```

#### 2 Visualization of data

Initial exploratory analysis can be performed to understand the data set. For example,

1.  the proportion of employees with different job titles (or any other possible factor) for status of "attrition" and "non-attrition" may vary, and this can be plotted as follows. People titled "Laboratory Technician", "Sales Executive", and "Research Scientist" are among the top 3 groups that exhibit highest attrition rate.

```{r}

ggplot(df, aes(JobRole, fill=Attrition)) +
  geom_bar(aes(y=(..count..)/sum(..count..)), position="dodge") +
  scale_y_continuous(labels=percent) +
  xlab("Job Role") +
  ylab("Percentage")
     

```

2.  monthly income, job level, and service year may affect decision of leaving for employees in different departments. For example, junior staffs with lower pay will be more likely to leave compared to those who are paid higher.

```{r}
ggplot(filter(df, (YearsAtCompany >= 2) & (YearsAtCompany <= 5) & (JobLevel < 3)),
       aes(x=factor(JobRole), y=MonthlyIncome, color=factor(Attrition))) +
  geom_boxplot() +
  xlab("Department") +
  ylab("Monthly income") +
  scale_fill_discrete(guide=guide_legend(title="Attrition")) +
  theme_bw() +
  theme(text=element_text(size=13), legend.position="top")
```

3.  Promotion is a commonly adopted HR strategy for employee retention. It can be observed in the following plot that for a certain department, e.g., Research & Development, employees with higher job level is more likely to leave if there are years since their last promotion.

```{r}
ggplot(filter(df, as.character(Attrition) == "Yes"), aes(x=YearsSinceLastPromotion)) +
  geom_histogram(binwidth=0.5) +
  aes(y=..density..) +
  xlab("Years since last promotion.") +
  ylab("Density") +
  # scale_fill_discrete(guide=guide_legend(title="Attrition")) +
  facet_grid(Department ~ JobLevel)
```

#### Data pre-processing

```{r}
# get predictors that has no variation.

pred_no_var <- names(df[, nearZeroVar(df)]) %T>% print()
```

```{r}

# remove the zero variation predictor columns.

df %<>% select(-one_of(pred_no_var))
```

Integer types of predictors which are nominal are converted to categorical type.

```{r}
# convert certain integer variable to factor variable.

int_2_ftr_vars <- c("Education", "EnvironmentSatisfaction", "JobInvolvement", "JobLevel", "JobSatisfaction", "NumCompaniesWorked", "PerformanceRating", "RelationshipSatisfaction", "StockOptionLevel")

df[, int_2_ftr_vars] <- lapply((df[, int_2_ftr_vars]), as.factor)

```

The variables of character type are converted to categorical type.

\

```{r}
df %<>% mutate_if(is.character, as.factor)
     

```

```{r}
str(df)
```

#### Problem formalization

After the data is well prepared, a model can be constructed for attrition prediction. Normally employee attrition prediction is categorized as a binary classification problem, i.e., to predict *whether or not an employee will leave*.

In this study case, the label for prediction is employee status, named as `Attrition` in the data set, which has two levels, `Yes` and `No`, indicating that the employee has left or stayed.

Check the label column to make sure it is a factor type, as the model to be built is a classifier.

```{r}

is.factor(df$Attrition)
```

It is possible that not all variables are correlated with the label, feature selection is therefore performed to filter out the most relevant ones.

As the data set is a blend of both numerical and discrete variables, certain correlation analysis (e.g., Pearson correlation) is not applicable. One alternative is to train a model and then rank the variable importance so as to select the most salient ones.

The following shows how to achieve variable importance ranking with a random forest model.

```{r}
# set up the training control.

control <- trainControl(method="repeatedcv", number=3, repeats=1)

# train the model

model <- train(dplyr::select(df, -Attrition), 
               df$Attrition,
               data=df, 
               method="rf", 
               preProcess="scale", 
               trControl=control)
```

```{r}

# estimate variable importance

imp <- varImp(model, scale=FALSE)

# plot

plot(imp)
```

```{r}
# select the top-ranking variables.

imp_list <- rownames(imp$importance)[order(imp$importance$Overall, decreasing=TRUE)]

# drop the low ranking variables. Here the last 3 variables are dropped. 

top_var <- 
  imp_list[1:(ncol(df) - 3)] %>%
  as.character() 

top_var
     
```

```{r}
# select the top ranking variables 

df %<>% select(., one_of(c(top_var, "Attrition")))
```

#### 2.1.6 Resampling

A prediction model can be then created for predictive analysis. The whole data is split into training and testing sets. The former is used for model creation while the latter for verification.

```{r}
train_index <- 
  createDataPartition(df$Attrition,
                      times=1,
                      p=.7) %>%
  unlist()

df_train <- df[train_index, ]
df_test <- df[-train_index, ]
     
```

One thing worthnoting is that the training set is not balanced, which may deteriorate the performance in training a model.

```{r}
table(df_train$Attrition)
```

Active employees (864) are more than terminated employees (166). There are several ways to deal with data imbalance issue:

1.  Resampling the data - either upsampling the minority class or downsampling the majority class.

2.  Use cost sensitive learning method.

In this case the first method is used. SMOTE is a commonly adopted method for synthetically upsampling minority class in an imbalanced data set. Package `DMwR` provides methods that apply SMOTE methods on training data set.

```{r}

# note DMwR::SMOTE does not handle well with tbl_df. Need to convert to data frame.
library("smotefamily")
library(ROSE)
df_train %<>% as.data.frame()
#ROSE(admit~., data = train, N = 500, seed=111)$data
df_train <- ROSE(Attrition ~ .,
                 data=df_train,
                  seed=111)$data


```

```{r}
table(df_train$Attrition)
```

#### Model building

After balancing the training set, a model can be created for prediction. For comparison purpose, different individual models, as well as ensemble of them, are trained on the data set. `caret` and `caretEnsemble` packages are used for training models.

1.  Individual models.

Three algorithms, support vector machine with radial basis function kernel, random forest, and extreme gradient boosting (xgboost), are used for model building.

\

```{r}


# initialize training control. 

tc <- trainControl(method="boot", 
                   number=3, 
                   repeats=3, 
                   search="grid",
                   classProbs=TRUE,
                   savePredictions="final",
                   summaryFunction=twoClassSummary)

# SVM model.

time_svm <- system.time(
  model_svm <- train(Attrition ~ .,
                     df_train,
                     method="svmRadial",
                     trainControl=tc)
)

# random forest model

time_rf <- system.time(
  model_rf <- train(Attrition ~ .,
                     df_train,
                     method="rf",
                     trainControl=tc)
)


```

```{r}
#Boosted Logistic Regression
ctrl <- trainControl(method = "repeatedcv",
                        number = 4,
                        savePredictions = TRUE,
                        verboseIter = T,
    
                                         returnResamp = "all")
time_logit <- system.time(
  model_logit <- train(Attrition ~ .,
                     df_train,
                     method="LogitBoost",
                     family="binomial",
                     trainControl=ctrl))

```

```{r}
time_ensemble <- system.time(
  model_list <- caretList(Attrition ~ ., 
                          data=df_train,
                          trControl=tc,
                          methodList=c("svmRadial", "rf","LogitBoost"
                                       ))
)
```

```{r}

# stack of models. Use glm for meta model.

model_stack <- caretStack(
  model_list,
  metric="ROC",
  method="glm",
  trControl=trainControl(
    method="boot",
    number=10,
    savePredictions="final",
    classProbs=TRUE,
    summaryFunction=twoClassSummary
  )
)

```

#### 2.1.8 Model validation

The trained models are applied on testing data sets for model evaluation.

```{r}
models <- list(model_svm, model_rf, model_stack,model_logit)

predictions <-lapply(models, 
                     predict, 
                     newdata=select(df_test, -Attrition))

     
```

```{r}
cm_metrics <- lapply(predictions,
                     confusionMatrix, 
                     reference=df_test$Attrition, 
                     positive="Yes")
cm_metrics
```

```{r}
# accuracy

acc_metrics <- 
  lapply(cm_metrics, `[[`, "overall") %>%
  lapply(`[`, 1) %>%
  unlist()

# recall

rec_metrics <- 
  lapply(cm_metrics, `[[`, "byClass") %>%
  lapply(`[`, 1) %>%
  unlist()
  
# precision

pre_metrics <- 
  lapply(cm_metrics, `[[`, "byClass") %>%
  lapply(`[`, 3) %>%
  unlist()

algo_list <- c("SVM RBF", "Random Forest", "Stacking"," Boosted Logistic Regression")
time_consumption <- c(time_svm[3], time_rf[3], time_ensemble[3],time_logit[3])

df_comp <- 
  data.frame(Models=algo_list, 
             Accuracy=acc_metrics, 
             Recall=rec_metrics, 
             Precision=pre_metrics,
             Time=time_consumption) %T>%
             {head(.) %>% print()}

```

```{r}

```
